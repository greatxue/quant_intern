{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044fc07f",
   "metadata": {},
   "source": [
    "**（1）代码部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b87a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f63b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'data-ML.csv'\n",
    "test_data_path = 'data_test.csv'\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb54b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "      <th>y</th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>...</th>\n",
       "      <th>x_19</th>\n",
       "      <th>x_20</th>\n",
       "      <th>x_21</th>\n",
       "      <th>x_22</th>\n",
       "      <th>x_23</th>\n",
       "      <th>x_24</th>\n",
       "      <th>x_25</th>\n",
       "      <th>x_26</th>\n",
       "      <th>x_27</th>\n",
       "      <th>x_28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31156</th>\n",
       "      <td>241</td>\n",
       "      <td>124</td>\n",
       "      <td>-0.127532</td>\n",
       "      <td>-0.108338</td>\n",
       "      <td>-0.108890</td>\n",
       "      <td>-0.107348</td>\n",
       "      <td>-0.108578</td>\n",
       "      <td>-0.656338</td>\n",
       "      <td>-0.704451</td>\n",
       "      <td>-0.508945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107590</td>\n",
       "      <td>-0.108307</td>\n",
       "      <td>-0.108331</td>\n",
       "      <td>-0.107160</td>\n",
       "      <td>-0.107703</td>\n",
       "      <td>-0.107611</td>\n",
       "      <td>-0.108556</td>\n",
       "      <td>-0.108372</td>\n",
       "      <td>-0.107686</td>\n",
       "      <td>-0.107492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31157</th>\n",
       "      <td>241</td>\n",
       "      <td>125</td>\n",
       "      <td>-0.418319</td>\n",
       "      <td>-0.205022</td>\n",
       "      <td>-0.203531</td>\n",
       "      <td>-0.204336</td>\n",
       "      <td>-0.202912</td>\n",
       "      <td>-0.748598</td>\n",
       "      <td>-0.787103</td>\n",
       "      <td>-0.701324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203608</td>\n",
       "      <td>-0.203424</td>\n",
       "      <td>-0.204418</td>\n",
       "      <td>-0.201016</td>\n",
       "      <td>-0.202098</td>\n",
       "      <td>-0.201792</td>\n",
       "      <td>-0.202753</td>\n",
       "      <td>-0.202379</td>\n",
       "      <td>-0.202881</td>\n",
       "      <td>-0.202239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31158</th>\n",
       "      <td>241</td>\n",
       "      <td>126</td>\n",
       "      <td>0.093849</td>\n",
       "      <td>-0.220328</td>\n",
       "      <td>-0.216690</td>\n",
       "      <td>-0.220113</td>\n",
       "      <td>-0.216993</td>\n",
       "      <td>0.455691</td>\n",
       "      <td>1.186600</td>\n",
       "      <td>0.133938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218690</td>\n",
       "      <td>-0.218992</td>\n",
       "      <td>-0.219554</td>\n",
       "      <td>-0.216605</td>\n",
       "      <td>-0.218371</td>\n",
       "      <td>-0.218316</td>\n",
       "      <td>-0.216912</td>\n",
       "      <td>-0.217480</td>\n",
       "      <td>-0.218867</td>\n",
       "      <td>-0.218569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31159</th>\n",
       "      <td>241</td>\n",
       "      <td>127</td>\n",
       "      <td>-0.540083</td>\n",
       "      <td>-0.214812</td>\n",
       "      <td>-0.211793</td>\n",
       "      <td>-0.215237</td>\n",
       "      <td>-0.213479</td>\n",
       "      <td>1.353480</td>\n",
       "      <td>1.962952</td>\n",
       "      <td>0.305835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213855</td>\n",
       "      <td>-0.214036</td>\n",
       "      <td>-0.213563</td>\n",
       "      <td>-0.213043</td>\n",
       "      <td>-0.214655</td>\n",
       "      <td>-0.213908</td>\n",
       "      <td>-0.213250</td>\n",
       "      <td>-0.214016</td>\n",
       "      <td>-0.215259</td>\n",
       "      <td>-0.214204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31160</th>\n",
       "      <td>241</td>\n",
       "      <td>128</td>\n",
       "      <td>-1.282562</td>\n",
       "      <td>-0.059840</td>\n",
       "      <td>-0.060004</td>\n",
       "      <td>-0.058656</td>\n",
       "      <td>-0.060545</td>\n",
       "      <td>3.188366</td>\n",
       "      <td>3.758701</td>\n",
       "      <td>5.198595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055931</td>\n",
       "      <td>-0.056681</td>\n",
       "      <td>-0.059642</td>\n",
       "      <td>-0.052446</td>\n",
       "      <td>-0.057986</td>\n",
       "      <td>-0.057476</td>\n",
       "      <td>-0.060750</td>\n",
       "      <td>-0.054918</td>\n",
       "      <td>-0.058649</td>\n",
       "      <td>-0.057281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  code         y       x_0       x_1       x_2       x_3       x_4  \\\n",
       "31156   241   124 -0.127532 -0.108338 -0.108890 -0.107348 -0.108578 -0.656338   \n",
       "31157   241   125 -0.418319 -0.205022 -0.203531 -0.204336 -0.202912 -0.748598   \n",
       "31158   241   126  0.093849 -0.220328 -0.216690 -0.220113 -0.216993  0.455691   \n",
       "31159   241   127 -0.540083 -0.214812 -0.211793 -0.215237 -0.213479  1.353480   \n",
       "31160   241   128 -1.282562 -0.059840 -0.060004 -0.058656 -0.060545  3.188366   \n",
       "\n",
       "            x_5       x_6  ...      x_19      x_20      x_21      x_22  \\\n",
       "31156 -0.704451 -0.508945  ... -0.107590 -0.108307 -0.108331 -0.107160   \n",
       "31157 -0.787103 -0.701324  ... -0.203608 -0.203424 -0.204418 -0.201016   \n",
       "31158  1.186600  0.133938  ... -0.218690 -0.218992 -0.219554 -0.216605   \n",
       "31159  1.962952  0.305835  ... -0.213855 -0.214036 -0.213563 -0.213043   \n",
       "31160  3.758701  5.198595  ... -0.055931 -0.056681 -0.059642 -0.052446   \n",
       "\n",
       "           x_23      x_24      x_25      x_26      x_27      x_28  \n",
       "31156 -0.107703 -0.107611 -0.108556 -0.108372 -0.107686 -0.107492  \n",
       "31157 -0.202098 -0.201792 -0.202753 -0.202379 -0.202881 -0.202239  \n",
       "31158 -0.218371 -0.218316 -0.216912 -0.217480 -0.218867 -0.218569  \n",
       "31159 -0.214655 -0.213908 -0.213250 -0.214016 -0.215259 -0.214204  \n",
       "31160 -0.057986 -0.057476 -0.060750 -0.054918 -0.058649 -0.057281  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5ec6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31161, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f56824b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>...</th>\n",
       "      <th>x_19</th>\n",
       "      <th>x_20</th>\n",
       "      <th>x_21</th>\n",
       "      <th>x_22</th>\n",
       "      <th>x_23</th>\n",
       "      <th>x_24</th>\n",
       "      <th>x_25</th>\n",
       "      <th>x_26</th>\n",
       "      <th>x_27</th>\n",
       "      <th>x_28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.780489</td>\n",
       "      <td>3.971441</td>\n",
       "      <td>3.892044</td>\n",
       "      <td>3.995554</td>\n",
       "      <td>4.278905</td>\n",
       "      <td>3.627402</td>\n",
       "      <td>2.416898</td>\n",
       "      <td>1.895266</td>\n",
       "      <td>...</td>\n",
       "      <td>3.839883</td>\n",
       "      <td>3.850517</td>\n",
       "      <td>3.887270</td>\n",
       "      <td>3.773767</td>\n",
       "      <td>3.793691</td>\n",
       "      <td>3.826716</td>\n",
       "      <td>3.990291</td>\n",
       "      <td>3.831193</td>\n",
       "      <td>3.814994</td>\n",
       "      <td>3.835216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.159670</td>\n",
       "      <td>-0.159482</td>\n",
       "      <td>-0.159243</td>\n",
       "      <td>-0.158023</td>\n",
       "      <td>-0.763510</td>\n",
       "      <td>-0.814598</td>\n",
       "      <td>-0.831288</td>\n",
       "      <td>-0.789755</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158495</td>\n",
       "      <td>-0.159493</td>\n",
       "      <td>-0.160545</td>\n",
       "      <td>-0.157573</td>\n",
       "      <td>-0.157757</td>\n",
       "      <td>-0.158640</td>\n",
       "      <td>-0.157980</td>\n",
       "      <td>-0.157772</td>\n",
       "      <td>-0.158417</td>\n",
       "      <td>-0.158787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.045428</td>\n",
       "      <td>-0.045512</td>\n",
       "      <td>-0.042968</td>\n",
       "      <td>-0.042670</td>\n",
       "      <td>-0.549088</td>\n",
       "      <td>-0.541655</td>\n",
       "      <td>-0.624320</td>\n",
       "      <td>-0.370413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041661</td>\n",
       "      <td>-0.045551</td>\n",
       "      <td>-0.045309</td>\n",
       "      <td>-0.046442</td>\n",
       "      <td>-0.044814</td>\n",
       "      <td>-0.048065</td>\n",
       "      <td>-0.042872</td>\n",
       "      <td>-0.045184</td>\n",
       "      <td>-0.044059</td>\n",
       "      <td>-0.047205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.175660</td>\n",
       "      <td>-0.177391</td>\n",
       "      <td>-0.179047</td>\n",
       "      <td>-0.177343</td>\n",
       "      <td>3.885955</td>\n",
       "      <td>5.151858</td>\n",
       "      <td>5.095823</td>\n",
       "      <td>5.862515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174667</td>\n",
       "      <td>-0.175792</td>\n",
       "      <td>-0.179078</td>\n",
       "      <td>-0.180495</td>\n",
       "      <td>-0.177532</td>\n",
       "      <td>-0.185748</td>\n",
       "      <td>-0.177268</td>\n",
       "      <td>-0.179197</td>\n",
       "      <td>-0.175522</td>\n",
       "      <td>-0.184364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.094343</td>\n",
       "      <td>-0.098661</td>\n",
       "      <td>-0.102046</td>\n",
       "      <td>-0.102826</td>\n",
       "      <td>1.861440</td>\n",
       "      <td>1.621870</td>\n",
       "      <td>1.595707</td>\n",
       "      <td>1.272282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097124</td>\n",
       "      <td>-0.085427</td>\n",
       "      <td>-0.097757</td>\n",
       "      <td>-0.096827</td>\n",
       "      <td>-0.094184</td>\n",
       "      <td>-0.082397</td>\n",
       "      <td>-0.102801</td>\n",
       "      <td>-0.098022</td>\n",
       "      <td>-0.095919</td>\n",
       "      <td>-0.083789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date  code       x_0       x_1       x_2       x_3       x_4       x_5  \\\n",
       "0     0     0  3.780489  3.971441  3.892044  3.995554  4.278905  3.627402   \n",
       "1     0     1 -0.159670 -0.159482 -0.159243 -0.158023 -0.763510 -0.814598   \n",
       "2     0     2 -0.045428 -0.045512 -0.042968 -0.042670 -0.549088 -0.541655   \n",
       "3     0     3 -0.175660 -0.177391 -0.179047 -0.177343  3.885955  5.151858   \n",
       "4     0     4 -0.094343 -0.098661 -0.102046 -0.102826  1.861440  1.621870   \n",
       "\n",
       "        x_6       x_7  ...      x_19      x_20      x_21      x_22      x_23  \\\n",
       "0  2.416898  1.895266  ...  3.839883  3.850517  3.887270  3.773767  3.793691   \n",
       "1 -0.831288 -0.789755  ... -0.158495 -0.159493 -0.160545 -0.157573 -0.157757   \n",
       "2 -0.624320 -0.370413  ... -0.041661 -0.045551 -0.045309 -0.046442 -0.044814   \n",
       "3  5.095823  5.862515  ... -0.174667 -0.175792 -0.179078 -0.180495 -0.177532   \n",
       "4  1.595707  1.272282  ... -0.097124 -0.085427 -0.097757 -0.096827 -0.094184   \n",
       "\n",
       "       x_24      x_25      x_26      x_27      x_28  \n",
       "0  3.826716  3.990291  3.831193  3.814994  3.835216  \n",
       "1 -0.158640 -0.157980 -0.157772 -0.158417 -0.158787  \n",
       "2 -0.048065 -0.042872 -0.045184 -0.044059 -0.047205  \n",
       "3 -0.185748 -0.177268 -0.179197 -0.175522 -0.184364  \n",
       "4 -0.082397 -0.102801 -0.098022 -0.095919 -0.083789  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2b3443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10439, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6430c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.iloc[:, 3:].replace(0, pd.NA)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "\n",
    "y_train = train_data.iloc[:, 2].to_frame().replace(0, pd.NA)\n",
    "y_train = y_train.fillna(y_train.mean())\n",
    "\n",
    "X_test = test_data.iloc[:, 2:].replace(0, pd.NA)\n",
    "X_test = X_test.fillna(X_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b6a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77c887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76050cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1)) \n",
    " \n",
    "X_train = scaler.fit_transform(X_train)  \n",
    "y_train = scaler.fit_transform(y_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "#scaler = StandardScaler()  \n",
    "X_train = scaler.fit_transform(X_train)  \n",
    "y_train = scaler.fit_transform(y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c14466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d439a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "torch_dataset = torch.utils.data.TensorDataset(X_train, y_train)  \n",
    "\n",
    "#这里日期被脱敏，所以我用了基本的分隔方式；只要有索引就可以根据需要特定分割。\n",
    "torch.manual_seed(seed=2020) \n",
    "train, validation = torch.utils.data.random_split(torch_dataset, [11161, 20000])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddcf53ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6932820081710815\n",
      "Epoch 2, Loss: 0.528477668762207\n",
      "Epoch 3, Loss: 0.13256670534610748\n",
      "Epoch 4, Loss: 0.08730072528123856\n",
      "Epoch 5, Loss: 0.08226903527975082\n",
      "Epoch 6, Loss: 0.05253587290644646\n",
      "Epoch 7, Loss: 0.06567157804965973\n",
      "Epoch 8, Loss: 0.0623103529214859\n",
      "Epoch 9, Loss: 0.042090319097042084\n",
      "Epoch 10, Loss: 0.028348775580525398\n",
      "Epoch 11, Loss: 0.023993775248527527\n",
      "Epoch 12, Loss: 0.025548027828335762\n",
      "Epoch 13, Loss: 0.03093358688056469\n",
      "Epoch 14, Loss: 0.02971429005265236\n",
      "Epoch 15, Loss: 0.026814715936779976\n",
      "Epoch 16, Loss: 0.02819668874144554\n",
      "Epoch 17, Loss: 0.017304575070738792\n",
      "Epoch 18, Loss: 0.021278977394104004\n",
      "Epoch 19, Loss: 0.017577312886714935\n",
      "Epoch 20, Loss: 0.01614570803940296\n",
      "Epoch 21, Loss: 0.020704416558146477\n",
      "Epoch 22, Loss: 0.01979103870689869\n",
      "Epoch 23, Loss: 0.01506909541785717\n",
      "Epoch 24, Loss: 0.022787747904658318\n",
      "Epoch 25, Loss: 0.01943368650972843\n",
      "Epoch 26, Loss: 0.012533999048173428\n",
      "Epoch 27, Loss: 0.016152113676071167\n",
      "Epoch 28, Loss: 0.011880282312631607\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "\n",
    "input_dim = X_train.shape[1] \n",
    "model = MLP(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) \n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) \n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, epochs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e444bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "\n",
    "y_prediction = scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62f8f6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43436307],\n",
       "       [0.46450618],\n",
       "       [0.46679857],\n",
       "       ...,\n",
       "       [0.46455109],\n",
       "       [0.46432781],\n",
       "       [0.43754569]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b95ce8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('y_prediction.csv', y_prediction, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3bc72",
   "metadata": {},
   "source": [
    "**（2）执行超参数搜索：**\n",
    "\n",
    "+ 首先，常见的超参数包括学习率、批大小、网络层数、每层的神经元数量等，我们需要确定它们取值范围或候选值；\n",
    "\n",
    "+ 其次，我们用网格搜索、随机搜索或基于梯度的方法（如Hyperband、Optuna等）遍历超参数空间；\n",
    "\n",
    "+ 最后，对于每一组超参数，使用训练数据训练模型，并使用验证集评估模型的性能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
